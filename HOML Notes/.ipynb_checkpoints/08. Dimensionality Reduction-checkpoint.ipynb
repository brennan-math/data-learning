{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd458f93-2236-4bf8-810b-f7ebdffbb431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "from sklearn.random_projection import johnson_lindenstrauss_min_dim, GaussianRandomProjection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7430af1-c363-417d-b20f-79f15d2ca6df",
   "metadata": {},
   "source": [
    "# The Curse of Dimensionality\n",
    "\n",
    "Training sets with high dimensions are resource hogs when training a model, but reducing the dimensions can negatively impact performance.\n",
    "\n",
    "Additionally, these training sets are very sparse in their space because there is a lot of distance between points in higher dimensions. The number of training instances required to reach a given density grows exponentially with the number of dimensions. With just 100 features, all ranging from $0$ to $1$, you would need more training instances than atoms in the observable universe in order for training instances to be within $0.1$ of each other on average, assuming they were spread out uniformly across all dimensions.\n",
    "\n",
    "In most data sets, training instances are not spread out uniformly across all dimensions. Many features will be highly correlated or are almost constant. As a result, the training instances lie within (or close to) a much lower-dimensional subspace of the high-dimensional space. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a08dae0-d4a1-4470-a595-b92589ad3a7d",
   "metadata": {},
   "source": [
    "# Principle Component Analysis (PCA)\n",
    "\n",
    "First, PCA identifies the hyperplane closest to the data and then projects the data onto it. PCA identifies the $1$-dimensional subspace that has the largest amount of variance in the training set. Then it finds a $1$-dimensional subspace that is orthogonal to the irst one and that accounts for the largest amount of the remaining variance. It continues until a hyperplane is found. The $i^{th}$ subspace is called the _$i^{th}$ principal component_ of the data.\n",
    "\n",
    "_Singular Value Decomposition (SVD)_ \\\n",
    "A matrix decomposition of a matrix $X$ into $U\\Sigma V^{T}$ where $V$ contains the unit vectors that define all the principal components."
   ]
  },
  {
   "cell_type": "raw",
   "id": "07c2b127-e41c-4b61-84a2-9999370c4a15",
   "metadata": {},
   "source": [
    "pca = PCA(n_components = n)\n",
    "X_pca = pca.fit_transform(X)\n",
    "## If n is an integer, pca reduces to that dimension subspace.\n",
    "## If n is a float between 0 and 1, pca views this as a percent and reduces to the number of dimensions necessary to keep at least that percent of variance.\n",
    "\n",
    "pca.explained_variance_ratio_   ## Outputs the percent of variance along each 1-dimensional subspace\n",
    "pca.n_components_               ## The dimension the training set is reduced to\n",
    "\n",
    "X_recovered = pca.inverse_transform(X_pca)\n",
    "## Decompress the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b0d9e7-e54a-4d87-8b06-d4b39ef427fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
